# Multi-Dataset Ingestion System

This document outlines the implementation of the advanced multi-dataset ingestion system for the AI E-Invoicing platform.

## Overview

To improve model training and zero-template extraction, we have integrated multiple high-quality Hugging Face datasets into our pipeline. The system provides a unified entry point to ingest, normalize, and deduplicate data from diverse sources.

### Integrated Datasets

- **Voxel51/high-quality-invoice-images-for-ocr**: High-fidelity invoices with structured ground truth annotations. Used for high-confidence training.
- **mychen76/invoices-and-receipts_ocr_v1**: Diverse set of invoices and receipts. Useful for improving robustness across messy layouts.
- **GokulRajaR/invoice-ocr-json**: Large-scale dataset with labels generated by GPT-4o.

## Key Features

### 1. Canonical Schema
All incoming data is normalized into a `CanonicalInvoiceSchema`. This ensures that regardless of the source dataset's layout or naming convention (e.g., `seller` vs `vendor_name`), the data is stored consistently in our database.

### 2. Intelligent Deduplication
The pipeline uses **Perceptual Hashing (pHash)** via the `image-hash` library. This allows the system to identify identical or near-identical invoice images across different datasets, preventing duplicate records from being ingested.

### 3. Modular Adapter Architecture
Each dataset has a dedicated adapter class:
- **`Voxel51Adapter`**: Handles local metadata parsing and automatic image downloads.
- **`Mychen76Adapter`**: Normalizes complex nested JSON strings and handles receipt-specific fields.
- **`GokulRajaAdapter`**: Parses GPT-generated JSON labels with safety guards.

### 4. Database Provenance
The database schema has been extended to track the origin of every record:
- `source_dataset`: The name of the original HF dataset.
- `source_id`: The unique identifier in the source.
- `annotation_confidence`: Estimated quality of the labels (Ground Truth vs Model Generated).
- `is_training_data`: Flag to separate processed business invoices from training datasets.

## Implementation Details

### Core Files
- `ingestion/hf_loader.py`: Entry point CLI script.
- `ingestion/hf_datasets/pipeline.py`: Orchestrator for the ingestion process.
- `ingestion/hf_datasets/base_adapter.py`: Abstract base class for all dataset loaders.
- `ingestion/hf_datasets/*_adapter.py`: Specialized dataset loaders.

### Unified Line Item Table
Training data requires granular line-item labels. We added the `invoice_line_items` table to store structured descriptions, quantities, and prices, moving away from simple nested JSON for these critical training fields.

## Usage

Run the ingestion pipeline with a limit per dataset:

```bash
PYTHONPATH=. python ingestion/hf_loader.py --limit 50
```

The system will:
1. Load records from all configured adapters.
2. Perform cross-dataset image hashing.
3. Skip existing duplicates in the database.
4. Bulk-insert unique records and their structured line items.
